llm:
  provider: "gemini"  # The LLM provider to use
  model: "gemini/gemini-1.5-pro"     # The model to use
  temperature: 0.7   # Controls randomness in the output (0.0 to 1.0)
  max_tokens: 8192   # Maximum number of tokens in the response

tools:
  serper_api_key: "${SERPER_API_KEY}"  # For web search
  github_token: "${GITHUB_TOKEN}"       # For GitHub operations

cache:
  enabled: true
  directory: ".cache"
  ttl: 3600  # Time to live in seconds

logging:
  level: "INFO"
  file: "app.log" 